## dubbo配置
#指定当前服务/应用的名字（同样的服务名字相同，不要和别的服务同名）
dubbo.application.name = backend.scaffold.xiaoya-project-scaffold-provider
dubbo.application.id=backend.scaffold.xiaoya-project-scaffold-provider
demo.service.version = 1.0.0
dubbo.protocol.name = dubbo
dubbo.protocol.port = 20880
#指定注册中心的位置
dubbo.registry.address = zookeeper://47.110.244.136:2181
#统一设置服务提供方的规则
dubbo.provider.timeout = 1000


## mysql数据库配置
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
#spring.datasource.name = mytest 多数据源时可配
spring.datasource.url=jdbc:mysql://rm-bp1oe6xy100ja45ario.mysql.rds.aliyuncs.com:3306/xydev?useUnicode=true&characterEncoding=UTF-8
spring.datasource.username=xydev
spring.datasource.password=xydev@123
spring.datasource.max-active=10
spring.datasource.max-idle=5
spring.datasource.min-idle=0


## druid配置
# 下面为连接池的补充设置，应用到上面所有数据源中
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
# 初始化大小，最小，最大
spring.datasource.initialSize=1
spring.datasource.minIdle=3
spring.datasource.maxActive=20
# 配置获取连接等待超时的时间
spring.datasource.maxWait=60000
# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
spring.datasource.timeBetweenEvictionRunsMillis=60000
# 配置一个连接在池中最小生存的时间，单位是毫秒
spring.datasource.minEvictableIdleTimeMillis=30000
spring.datasource.validationQuery=select 'x'
spring.datasource.testWhileIdle=true
spring.datasource.testOnBorrow=false
spring.datasource.testOnReturn=false
# 打开PSCache，并且指定每个连接上PSCache的大小
spring.datasource.poolPreparedStatements=true
spring.datasource.maxPoolPreparedStatementPerConnectionSize=20
# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
spring.datasource.filters=stat,wall,slf4j
# 通过connectProperties属性来打开mergeSql功能；慢SQL记录
spring.datasource.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
# 合并多个DruidDataSource的监控数据
spring.datasource.useGlobalDataSourceStat=true


## sharding sphere配置
spring.shardingsphere.datasource.names=ds0,ds1

# 数据源
spring.shardingsphere.datasource.ds0.type=com.alibaba.druid.pool.DruidDataSource
spring.shardingsphere.datasource.ds0.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.ds0.url=jdbc:mysql://localhost:3306/ds_15?characterEncoding=utf-8
spring.shardingsphere.datasource.ds0.username=root
spring.shardingsphere.datasource.ds0.password=123456

spring.shardingsphere.datasource.ds1.type=com.alibaba.druid.pool.DruidDataSource
spring.shardingsphere.datasource.ds1.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.ds1.url=jdbc:mysql://localhost:3306/ds_16?characterEncoding=utf-8
spring.shardingsphere.datasource.ds1.username=root
spring.shardingsphere.datasource.ds1.password=123456

# 分表配置
spring.shardingsphere.sharding.tables.user.actual-data-nodes=ds$->{0..1}.user_$->{0..2}
spring.shardingsphere.sharding.tables.user.table-strategy.inline.sharding-column=id
spring.shardingsphere.sharding.tables.user.table-strategy.inline.algorithm-expression=user_$->{id % 3}
spring.shardingsphere.sharding.tables.user.key-generator.column=id
spring.shardingsphere.sharding.tables.user.key-generator.type=SNOWFLAKE

# 分库配置
spring.shardingsphere.sharding.default-database-strategy.inline.sharding-column=id
spring.shardingsphere.sharding.default-database-strategy.inline.algorithm-expression=ds$->{id % 2}

# 不分库分表的数据源指定
#spring.shardingsphere.sharding.default-data-source-name=ds0

# 广播表，每个节点复制一份，适用于配置类的数据
#spring.shardingsphere.sharding.broadcast-tables=loudong

# 显示SQL
spring.shardingsphere.props.sql.show=true


## mybatis配置
mybatis.mapper-locations=classpath*:mapping/**/*Mapper.xml
## 打印sql日志（二者选一即可）
#mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl
logging.level.com.xiaoya.scaffold.provider.mapper=DEBUG

## mybatis分页插件
pagehelper.helper-dialect=mysql
pagehelper.params=count=countSql
pagehelper.reasonable=true
pagehelper.support-methods-arguments=true

## kafka配置
# 指定kafka 代理地址，可以多个
#spring.kafka.bootstrap-servers=192.168.106.239:9092,192.168.106.240:9092,192.168.106.241:9092
spring.kafka.bootstrap-servers=39.100.62.143:30002,39.100.62.143:30003,39.100.62.143:30004
#spring.kafka.bootstrap-servers=127.0.0.1:9092
#发送失败重试次数
spring.kafka.producer.retries=0
# 每次批量发送消息的数量
spring.kafka.producer.batch-size=16384
#32MB的批处理缓冲区
spring.kafka.producer.buffer-memory=33554432
#消费者群组ID，发布-订阅模式，即如果一个生产者，多个消费者都要消费，那么需要定义自己的群组，同一群组内的消费者只有一个能消费到消息
spring.kafka.consumer.group-id=test-consumer-group
#关闭自动提交offset，spring会手工提交offset
spring.kafka.consumer.enable-auto-commit=false
#key-value序列化反序列化
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

## redis配置
#spring.redis.cache.nodes=127.0.0.1:7001,127.0.0.1:7002,127.0.0.1:7003,127.0.0.1:7004,127.0.0.1:7005,127.0.0.1:7006
#spring.redis.cache.host=localhost:6379
spring.redis.cache.nodes=39.100.59.54:30002
spring.redis.cache.host=39.100.59.54:30002
spring.redis.cache.password=
spring.redis.cache.maxWaitMillis=5000
spring.redis.cache.maxIdle=
spring.redis.cache.minIdle=
spring.redis.cache.maxTotal=

## log4j2配置
#配置日志级别
#logging.level.root=debug
#日志配置文件路径
logging.config=classpath:log4j2.xml
logging.level.root=INFO
logging.level.org.springframework.boot.autoconfigure=INFO